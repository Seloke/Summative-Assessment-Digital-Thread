{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital For Industrial Summative - Part 1\n",
    " \n",
    "## Creating A Digital Thread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Digital Thread is the one unifying theme or characteristic that connects every aspect of an asset or unit, right from its inception and design, to manufacture, deployment, operations, maintenance to eventual retirement.\n",
    "\n",
    "In analysis, a digital thread is the logical with which we bind and merge the various data sources into one whole, so that it lends itself to quantitative approaches easily.\n",
    "\n",
    "<img src = 'images/Digital_Thread.JPG' width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A digital thread is a technique to 'stitch' the data that comes in disjoint tables, such that they can be put together logically. That is a task for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sets provided:\n",
    "\n",
    "We have been give 5 data sets, all of which related to one month's worth of readings taken at a live volcano site. The volcano was instrumented with multiple sensors in 10 different geographical points (nodes). Our goal is to combine and merge all of this into one digital thread, making it amenable for analysis.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "    1.0 Read all the needed input files\n",
    "    2.0 Plotting Sensor Time Series\n",
    "    3.0 Descriptive Analysis One data frame at a time \n",
    "    4.0 Creating a Digital Thread from the data sets\n",
    "    5.0 Time Series based analysis\n",
    "    6.0 Correlations Analysis\n",
    "    7.0 Data Manipulations to Merge multiple data sets\n",
    "    8.0 Building A Battery Remaining-Life prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Read all the needed input files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create multiple data frames, one to hold each data table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all Data frame time stamps into date-time format, so that time-based indexing is possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/Volcano'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_files = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Plotting Sensor Time Series\n",
    "\n",
    "Create separate plots for each sensor in the dataset. Store each one in a separate file.\n",
    "\n",
    "**What (if anything) can we tell about the various sensors from the plots?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Descriptive Analysis One data frame at a time **\n",
    "\n",
    "3.1: How many sensors of each type are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2.type_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### 4.0 Creating a Digital Thread from the data sets**\n",
    "\n",
    "**Goal: Merge everything into one wide data table ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have two data frames: `sensors` and `types` Merge each sensors with its type. (Hint: Use `pd.merge()`)\n",
    "\n",
    "Question: What does this achieve? Why should we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take the raw `data` data frame and merge it with all the `sensors` and their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.merge(data, s2, left_on='sensor_id', right_on='id_x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Time Series based analysis\n",
    "\n",
    "Now, we are going to take this stitiched data frame and use it for our analysis. Specifically, we are going to perform Time-based analysis on this data.\n",
    "\n",
    "    Step 1: Take the `full` data frame and make the Time Stamp datetime format\n",
    "    Step 2: Resample the dataset to the daily level. (One observation per date)\n",
    "    Step 3: For the resampled data, calculate the daily mean, min and max values for each sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resample to get daily averages.\n",
    "Then subset to select the rows you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = full[full.type_id=='HUMA']['value'].resample('D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Correlations Analysis\n",
    "\n",
    "The next task is to perform a correlation analysis. \n",
    "\n",
    "1.  Goal: We want to find all the sensors that are strongly correlated to each other.\n",
    "2. One of the reasons for doing this is that if two (or more) sensors are very highly correlated, we only need to keep one out of each correlated set. (This reduces the problem size and also takes care of colinearlity-related unstability in certain calculations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Correlation heatmap that we are looking for should be along the following lines:\n",
    "\n",
    "\n",
    "<img src = \"images/Corr_plot.JPG\" width=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, look for correlations visually. \n",
    "\n",
    "**Subtask: Plot all the sensors values (of one type) over time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub task: Identify all the temperature sensors in the data set. Hint: These are the ones that have the string 'TCA' in their name id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = full.type_id == 'TCA'\n",
    "plt.figure(figsize=(20,10))\n",
    "full[criteria]['value'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_sensors = [s for s in s2[s2.type_id =='TCA']['id_x']]\n",
    "temperature_senors = [s for t,s in zip(s2['type_id'], s2['id_x']) if t=='TCA'] # alternative way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Using Pandas to plot all the temperature sensors on one single plot. **\n",
    "\n",
    "- Use pandas to loop over each temperature sensor, and plot them one at a time.\n",
    "- Hint: Use the following trick to do this is to plot one line at a time, over and over in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "for s in temp_sensors:\n",
    "    #print(s)\n",
    "    sub_df = full[full['sensor_id']==s]\n",
    "    plt.plot(sub_df.index, sub_df['value'], '.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is fine, but all the lines are too close together. We cannot see how each sensor is behaving. For that, we can try drawing \"Subplots.\" In these plots, each sensor gets its own plot (called a 'panel')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Subplots - Each sensor gets its own panel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axarr = plt.subplots(10, sharex=True)\n",
    "fig.set_size_inches(20,30) \n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for i,s in enumerate(temp_sensors):\n",
    "    sub_df = full[full['sensor_id']==s]\n",
    "    axarr[i].plot(sub_df.index, sub_df['value'], '.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Creating a reshaped Data Frame of just Temperature sensors**\n",
    "\n",
    "For this, we are going to have each column be 1 sensor... from 1 to 10. The rows will be timestamps, as before.\n",
    "\n",
    "Hint: pd.pivot() is perfect for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.pivot(index='patient', columns='obs', values='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = full[full['sensor_id'].isin(temp_sensors)]\n",
    "del temp_df['timestamp']\n",
    "temp_df = temp_df.reset_index()\n",
    "temp_df.pivot(columns='sensor_id', values='value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a real sensor data set, there are some time stamps and sensor_id's that are repeating. (Unfortunately, this happens often in real data sets.)\n",
    "\n",
    "**Task: Find all rows with the same [Timestamp, Sensor_id] and delete them **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's a clever way to find out all the duplicated rows.**\n",
    "\n",
    "Some Timestamp and sensor_id are repeating. That causes Indexing problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat(g for _, g in df.groupby(\"ID\") if len(g) > 1)\n",
    "pd.concat(g for _, g in temp_df.groupby(['timestamp', 'sensor_id']) if len(g) > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task: Drop all the rows where timestamp and sensor_id are duplicated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = temp_df.drop_duplicates(subset = ['timestamp', 'sensor_id'], keep='first')\n",
    "# Hint: Look at https://segment.com/blog/5-advanced-testing-techniques-in-go/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_df_cor = tdf.pivot(index='timestamp', columns='sensor_id', values='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#daily_temp_cor_df = temp_df_cor.resample('D').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are finally ready to calculate the correlations across sensor values. Hint: `Use corr()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_df = daily_temp_cor_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask to display only the lower triangle of the matrix (since it's mirrored around its \n",
    "# top-left to bottom-right diagonal).\n",
    "mask = np.zeros_like(corr_df)\n",
    "mask[np.triu_indices_from(mask)] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap using seaborn library. \n",
    "# List if colormaps (parameter 'cmap') is available here: http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "sns.heatmap(corr_df, cmap='RdYlGn_r', vmax=1.0, vmin=-1.0 , mask = mask, linewidths=2.5)\n",
    " \n",
    "# Show the plot we reorient the labels for each column and row to make them easier to read.\n",
    "plt.yticks(rotation=0) \n",
    "plt.xticks(rotation=90) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full2 = full.drop_duplicates(subset = ['timestamp', 'sensor_id'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_wide = full2.pivot(index='timestamp', columns='sensor_id', values='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_all_sensors = full_wide.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_all_sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying to building a Linear Model**\n",
    "\n",
    "In order to do that, we first need to create a data frame with the columns representing only those sensors for ONE NODE.\n",
    "\n",
    "Try to see if pd.pivot() can help with grouping Nodes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.name # so we have 10 \"nodes\" with 6 sensors each. [T, Pr, HUMA, PPM , PPM2, BATT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullnode = pd.merge(full, nodes, left_on='node_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullnode.columns, fullnode.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullnode.set_index('timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullnode.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fullnode.pivot_table(index=['timestamp', 'name'], columns='type', values='value')\n",
    "fn_wide = fullnode.pivot_table(index=['timestamp','name'], columns=['type'], values='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, let's make the Node ('name') into its own column. We do this by reset_index() for that level (=1)\n",
    "fn_wide.reset_index(level=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_wide.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can perform Linear Regression, we have one last step remaining. We'd like to \"resample\" all the data, aggregating it down to 'Daily' Levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmfn = fn_wide.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmfn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Stitching. \n",
    "\n",
    "**The Digital Thread for this dataset has been created**\n",
    "\n",
    "This \"digital Thread\" has been used to 'stitch' the data frame with all the values we wish to analyze.\n",
    "\n",
    "---\n",
    "Now we finally have the data frame in the shape we wanted to enable Linear Regression.\n",
    "\n",
    "### 8.0 Sample Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1 Building A Battery Remaining-Life prediction model **\n",
    "Build a machine learning model (LR, tree-based or any other) to try and predict the Battery life (remaining) as a function of any of the other sensor characteristic.\n",
    "\n",
    "* Which variable (sensor) is a good predictor of battery life?\n",
    "* Is your linear regression a \"good fit?\"\n",
    "* What it the RMSE of your predicted values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# Supervised learning linear regression\n",
    "#==============================================================================\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "train = lmfn[:-30]\n",
    "test = lmfn[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, predictors = 'battery', 'temperature'\n",
    "\n",
    "x_train=train[predictors].to_frame() #converts the pandas Series to numpy.ndarray\n",
    "y_train=train[target].to_frame()\n",
    "x_test=test[predictors].to_frame() #converts the pandas Series to numpy.ndarray\n",
    "y_test=test[target].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.- Create linear regression object\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# 3.- Train the model using the training sets\n",
    "regr.fit(x_train,y_train)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \",  float(regr.coef_))\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % np.mean((regr.predict(x_train) - y_train) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
